import os
import re
import wikipedia
import torch
import torch.nn.functional as F
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from datasets import Dataset
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters

MODEL_NAME = "distilgpt2"
FINETUNED_MODEL_DIR = "./finetuned_model"
BOT_TOKEN = ""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

wikipedia.set_lang("ru")  # –†—É—Å—Å–∫–∏–π —è–∑—ã–∫

# ======== –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –í–∏–∫–∏–ø–µ–¥–∏–∏ ========
def detect_question_type(text):
    text = text.lower()
    if any(w in text for w in ["–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º", "–∏–∑-–∑–∞", "–ø–æ—Ç–æ–º—É —á—Ç–æ"]):
        return "why"
    if any(w in text for w in ["–∫—Ç–æ", "–∫–µ–º", "—á–µ–π"]):
        return "who"
    if any(w in text for w in ["–∫–æ–≥–¥–∞", "–¥–∞—Ç–∞", "–≥–æ–¥", "—á–∏—Å–ª–æ"]):
        return "when"
    if any(w in text for w in ["–≥–¥–µ", "–∫—É–¥–∞", "–æ—Ç–∫—É–¥–∞"]):
        return "where"
    if any(w in text for w in ["–∫–∞–∫", "–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º"]):
        return "how"
    return "what"

def extract_reason(text):
    patterns = ["–ø–æ—Ç–æ–º—É —á—Ç–æ", "–∏–∑-–∑–∞", "—Ç–∞–∫ –∫–∞–∫", "–≤—Å–ª–µ–¥—Å—Ç–≤–∏–µ", "–ø—Ä–∏—á–∏–Ω–∞", "–ø–æ –ø—Ä–∏—á–∏–Ω–µ"]
    sentences = re.split(r"(?<=[.!?]) +", text)
    reasons = [s for s in sentences if any(p in s.lower() for p in patterns)]
    if reasons:
        return " ".join(reasons)
    return None

def extract_who(text):
    sentences = re.split(r"(?<=[.!?]) +", text)
    for s in sentences:
        if re.search(r"[–ê-–Ø–Å][–∞-—è—ë]+", s):
            return s
    return None

def extract_when(text):
    sentences = re.split(r"(?<=[.!?]) +", text)
    date_patterns = [r"\b\d{4}\b", r"\b\d{1,2} [–∞-—è]+ \d{4}\b"]
    for s in sentences:
        if any(re.search(p, s) for p in date_patterns):
            return s
    return None

def search_wikipedia(phrase):
    try:
        results = wikipedia.search(phrase)
        if not results:
            return None
        qtype = detect_question_type(phrase)
        for title in results:
            summary = wikipedia.summary(title, sentences=6)
            if qtype == "why":
                reason = extract_reason(summary)
                if reason:
                    return f"–ò–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏:\n{reason}"
            elif qtype == "who":
                who = extract_who(summary)
                if who:
                    return f"–ò–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏:\n{who}"
            elif qtype == "when":
                when = extract_when(summary)
                if when:
                    return f"–ò–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏:\n{when}"
            else:
                sentences = re.split(r"(?<=[.!?]) +", summary)
                return "–ò–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏:\n" + " ".join(sentences[:3])
        return "–ò–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏:\n" + wikipedia.summary(results[0], sentences=3)
    except Exception:
        return None

# ======== –ö–ª–∞—Å—Å —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∏ –æ–±—É—á–µ–Ω–∏–µ–º ========
class OnlineTrainer:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        if os.path.exists(FINETUNED_MODEL_DIR):
            self.model = AutoModelForCausalLM.from_pretrained(FINETUNED_MODEL_DIR)
            print("–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å")
        else:
            self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
            print("–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å distilgpt2")

        self.model.config.pad_token_id = self.tokenizer.pad_token_id
        self.model.to(device)

        self.data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=False)

    def generate_answer(self, prompt, max_length=100):
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True)
        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs["attention_mask"].to(device)

        outputs = self.model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_length=input_ids.shape[1] + max_length,
            pad_token_id=self.tokenizer.pad_token_id,
            do_sample=True,
            top_p=0.9,
            top_k=50,
            temperature=0.7,
            num_return_sequences=1,
            no_repeat_ngram_size=2
        )
        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = answer[len(prompt):].strip()

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        probs = self.get_token_probabilities(generated_text)

        return generated_text, probs

    def get_token_probabilities(self, text):
        """
        –ü—Ä–æ–≥–æ–Ω—è–µ–º —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å —Å labels=input_ids, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –ª–æ–≥–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Ç–æ–∫–µ–Ω–æ–≤.
        –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞.
        """
        encodings = self.tokenizer(text, return_tensors="pt")
        input_ids = encodings.input_ids.to(device)

        with torch.no_grad():
            outputs = self.model(input_ids, labels=input_ids)
            logits = outputs.logits  # (batch_size, seq_len, vocab_size)

        probs = torch.softmax(logits, dim=-1)

        token_probs = []
        for i in range(input_ids.size(1) - 1):
            token_id = input_ids[0, i + 1].item()
            prob = probs[0, i, token_id].item()
            token_probs.append(prob)

        return token_probs

    def fine_tune(self, question, answer, epochs=1):
        text = f"User: {question}\nBot: {answer}\n"
        encodings = self.tokenizer(text, return_tensors="pt", padding=True)
        dataset = Dataset.from_dict(encodings)

        training_args = TrainingArguments(
            output_dir=FINETUNED_MODEL_DIR,
            overwrite_output_dir=True,
            num_train_epochs=epochs,
            per_device_train_batch_size=1,
            save_steps=10,
            save_total_limit=2,
            logging_steps=5,
            logging_dir='./logs',
            no_cuda=(device.type != "cuda"),
        )

        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=dataset,
            data_collator=self.data_collator,
        )

        trainer.train()
        trainer.save_model(FINETUNED_MODEL_DIR)
        print("–ú–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

trainer = OnlineTrainer()
learning_users = {}
trained_questions = set()

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–ü—Ä–∏–≤–µ—Ç! –Ø –ò–ò-–±–æ—Ç —Å –ø–æ–∏—Å–∫–æ–º –ø–æ –í–∏–∫–∏–ø–µ–¥–∏–∏ –∏ –æ–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏–µ–º.")

async def message_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    text = update.message.text.strip()

    if user_id in learning_users:
        question = learning_users[user_id]
        answer = text
        await update.message.reply_text("–°–ø–∞—Å–∏–±–æ! –Ø –∑–∞–ø–æ–º–Ω–∏–ª –æ—Ç–≤–µ—Ç –∏ —Å–µ–π—á–∞—Å –æ–±—É—á—É—Å—å.")
        trainer.fine_tune(question, answer)
        await update.message.reply_text("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
        del learning_users[user_id]
        return

    wiki_answer = search_wikipedia(text)
    prompt = f"User: {text}\n"
    model_answer, token_probs = trainer.generate_answer(prompt)

    if token_probs:
        avg_prob = sum(token_probs) / len(token_probs)
    else:
        avg_prob = 0.0  # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π

    PROB_THRESHOLD = 0.5  # –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (—Å—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–∞)

    if avg_prob < PROB_THRESHOLD and wiki_answer:
        await update.message.reply_text(wiki_answer)
        if text not in trained_questions:
            trainer.fine_tune(text, wiki_answer)
            trained_questions.add(text)
        return

    if model_answer and len(model_answer) > 15:
        await update.message.reply_text(model_answer)
        if text not in trained_questions:
            trainer.fine_tune(text, model_answer)
            trained_questions.add(text)
        return

    await update.message.reply_text("–Ø –ø–æ–∫–∞ –Ω–µ –∑–Ω–∞—é, –∫–∞–∫ –æ—Ç–≤–µ—Ç–∏—Ç—å. –ö–∞–∫ –±—ã —Ç—ã –æ—Ç–≤–µ—Ç–∏–ª?")
    learning_users[user_id] = text

def main():
    app = ApplicationBuilder().token(BOT_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, message_handler))
    print("ü§ñ –ë–æ—Ç —Å –í–∏–∫–∏–ø–µ–¥–∏–µ–π –∏ –æ–±—É—á–µ–Ω–∏–µ–º –∑–∞–ø—É—â–µ–Ω!")
    app.run_polling()

if __name__ == "__main__":
    main()
